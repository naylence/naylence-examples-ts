# OpenTelemetry Example — Tracing with Explicit Configs

This example demonstrates how to integrate **OpenTelemetry** into a Naylence fabric. Unlike previous examples that relied on default configs, here we use **explicit configuration files** for the sentinel, agent, and client. This allows us to configure OpenTelemetry emitters alongside security and admission profiles.

Telemetry data is exported to an **OpenTelemetry Collector**, which forwards it to **Jaeger** for visualization. Authentication for telemetry export uses the same OAuth2 provider as admission, and the collector itself is protected by OIDC.

---

## What you'll learn

- Running a Naylence **sentinel**, **agent**, and **client** with explicit YAML configs (`sentinel-config.yml`, `agent-config.yml`, `client-config.yml`).
- Configuring **OpenTelemetry emitters** for each node.
- Exporting telemetry traces to an **OTel Collector** and visualizing them in **Jaeger**.
- Using OAuth2 tokens not only for admission but also for authenticating telemetry exports.
- How to configure the OpenTelemetry collector itself with OIDC protection.
- Working with self-signed certificates in a development environment.

---

## Components

- **docker-compose.yml** — orchestrates services:
  - **caddy** — reverse proxy, terminates TLS with internal CA.
  - **oauth2-server** — dev-only OAuth2 provider (client credentials flow).
  - **otel-collector** — receives traces, enforces OIDC, and exports them to Jaeger + debug output.
  - **jaeger** — UI for viewing traces.
  - **sentinel** — Naylence sentinel configured via `sentinel-config.yml`.
  - **math-agent** — RPC agent configured via `agent-config.yml`.
  - **client** — TypeScript client configured via `client-config.yml` and `.env.client`.

**Profiles in use:**

- Sentinel: `FAME_SECURITY_PROFILE=overlay`
- Agent: `FAME_ADMISSION_PROFILE=direct`
- Client: `FAME_ADMISSION_PROFILE=direct`

---

## Quick start

### Using Make

```bash
make start    # Generate secrets, build, and start all services
make run      # Execute the client (runs math operations)
```

This will:

1. Generate secrets and environment files.
2. Start Caddy, OAuth2 server, OTel collector, Jaeger, sentinel, and math agent.
3. Run the TypeScript client.

### Example client output

```
7
42
0 1 1 2 3 5 8 13 21 34
```

ℹ️ Traces will be available in **Jaeger UI** at [http://localhost:16686](http://localhost:16686).

---

## Viewing Traces in Jaeger

1. After running the client, open [http://localhost:16686](http://localhost:16686) in your browser.
2. Select **"naylence-telemetry"** from the Service dropdown.
3. Click **"Find Traces"** to see the distributed traces.
4. Click on a trace to see the detailed span timeline showing:
   - Client RPC calls
   - Sentinel routing
   - Agent operation execution
   - Network latency between components

---

## Telemetry Configuration

### Client/Agent/Sentinel (YAML config)

Each node explicitly declares a telemetry emitter. Example from `client-config.yml`:

```yaml
telemetry:
  type: OpenTelemetryTraceEmitter
  service_name: naylence-telemetry
  endpoint: "${env:FAME_TELEMETRY_OTLP_ENDPOINT:}"
  auth:
    type: BearerTokenHeaderAuth
    token_provider:
      type: OAuth2ClientCredentialsTokenProvider
      token_url: ${env:FAME_TELEMETRY_TOKEN_URL}
      client_id: ${env:FAME_TELEMETRY_CLIENT_ID}
      client_secret: ${env:FAME_TELEMETRY_CLIENT_SECRET}
      scopes:
        - node.connect
      audience: ${env:FAME_TELEMETRY_JWT_AUDIENCE}
```

This config ensures that telemetry spans are exported via OTLP to the collector, with OAuth2 client-credentials used to authenticate.

---

### OpenTelemetry Collector (server side)

The collector itself enforces OIDC authorization on incoming telemetry:

```yaml
extensions:
  oidc:
    issuer_url: https://oauth2-server
    audience: "fame.fabric"

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
        auth:
          authenticator: oidc
```

This configuration ensures only authenticated nodes can push telemetry data into the collector.

---

## Environment Variables Reference

Example `.env.client` (generated by `make init`):

```ini
# Custom config location so the runtime loads OpenTelemetry emitter settings
FAME_CONFIG=/etc/otel-config/client-config.yml

# Load FAME plugins to register factories
FAME_PLUGINS=@naylence/runtime,@naylence/agent-sdk

# SSL_CERT_FILE is required for the app to recognize caddy generated SSL certs
SSL_CERT_FILE=/data/caddy/pki/authorities/local/root.crt
NODE_EXTRA_CA_CERTS=/data/caddy/pki/authorities/local/root.crt

# Setting FAME_SECURITY_PROFILE to `overlay` enables overlay security features
FAME_SECURITY_PROFILE=overlay

# The profile to use for admission control
FAME_ADMISSION_PROFILE=direct

# WebSocket URL for direct admission to the sentinel
FAME_DIRECT_ADMISSION_URL=wss://sentinel/fame/v1/attach/ws/downstream

# OAuth2 token endpoint for admission authentication
FAME_ADMISSION_TOKEN_URL=https://oauth2-server/oauth/token

# OAuth2 client credentials for admission authentication
FAME_ADMISSION_CLIENT_ID=<YOUR_TEST_CLIENT_ID>
FAME_ADMISSION_CLIENT_SECRET=<YOUR_TEST_CLIENT_SECRET>

# JWT configuration for the client
FAME_JWT_AUDIENCE=fame.fabric
FAME_JWT_TRUSTED_ISSUER=https://oauth2-server

# Telemetry endpoint URL
FAME_TELEMETRY_OTLP_ENDPOINT=https://otel-collector/v1/traces
FAME_TELEMETRY_TOKEN_URL=https://oauth2-server/oauth/token
FAME_TELEMETRY_CLIENT_ID=<YOUR_TEST_CLIENT_ID>
FAME_TELEMETRY_CLIENT_SECRET=<YOUR_TEST_CLIENT_SECRET>
FAME_TELEMETRY_JWT_AUDIENCE=fame.fabric

OTEL_EXPORTER_OTLP_CERTIFICATE=/data/caddy/pki/authorities/local/root.crt
```

---

## Code comparison: Python vs TypeScript

Key differences in the implementation:

| Python                            | TypeScript                              | Notes                   |
| --------------------------------- | --------------------------------------- | ----------------------- |
| `math_agent.py`                   | `math-agent.ts`                         | File naming conventions |
| `SSL_CERT_FILE` only              | `SSL_CERT_FILE` + `NODE_EXTRA_CA_CERTS` | Node.js requires both   |
| `.env.client` sourced             | `--env-file config/.env.client`         | Docker env loading      |
| `python client.py`                | `node client.mjs`                       | Runtime execution       |
| YAML configs loaded automatically | Explicit `FAME_CONFIG` path required    | Config loading          |

---

## Architecture

```
┌─────────┐
│ Client  │──(OTLP/HTTPS + OAuth2)──┐
└─────────┘                          │
                                     ▼
┌──────────┐                  ┌────────────┐
│ Sentinel │──(OTLP/HTTPS)──▶ │ OTel       │
└──────────┘                  │ Collector  │──▶ Jaeger UI
                              │ (OIDC)     │
┌────────────┐                │            │
│ Math Agent │──(OTLP/HTTPS)─▶└────────────┘
└────────────┘

All HTTPS traffic goes through Caddy (TLS termination)
All telemetry exports are authenticated with OAuth2 tokens
```

---

## Troubleshooting

- **Client can't connect to sentinel**
  - Ensure all services are running: `docker compose ps`
  - Check sentinel logs: `docker compose logs sentinel`
  - Verify SSL certificates are generated: `docker compose logs caddy`

- **No traces in Jaeger**
  - Check OTel collector logs: `docker compose logs otel-collector`
  - Verify `FAME_TELEMETRY_OTLP_ENDPOINT` is correct in `.env.client`
  - Ensure OAuth2 credentials are valid for telemetry export
  - Try `make run-verbose` to see detailed logging

- **SSL/TLS certificate errors**
  - Wait for Caddy to generate certificates (check `docker compose logs caddy`)
  - Ensure `SSL_CERT_FILE` and `NODE_EXTRA_CA_CERTS` point to Caddy's root CA
  - Verify volume mounts for certificates are correct

- **OAuth2 authentication failures**
  - Check that secrets were generated: `ls config/secrets/`
  - Verify `.env.client` has correct `DEV_CLIENT_ID` and `DEV_CLIENT_SECRET`
  - Check oauth2-server logs: `docker compose logs oauth2-server`

- **Build errors**
  - Ensure Node.js 18+ is installed: `node --version`
  - Clean and rebuild: `make clean && make build`

- **Port conflicts**
  - Default ports: 16686 (Jaeger UI), 4317/4318 (OTel collector)
  - Change port mappings in `docker-compose.yml` if needed

---

## Makefile Commands

```bash
make build        # Install deps and compile TypeScript
make init         # Generate dev OAuth2 secrets (.env files)
make start        # Build image and start docker-compose stack
make stop         # Stop running stack
make run          # Execute the client inside the network
make run-verbose  # Execute the client with debug logging
make clean        # Remove generated files and volumes
make help         # Show available commands
```

---

## Notes

- This example shows how to integrate Naylence with **industry-standard observability tools**.
- The OTel collector here is dev-only; production setups should use hardened collector deployments.
- Jaeger is optional, you can configure other exporters (Grafana Tempo, Datadog, Zipkin, etc.).
- Securing Jaeger itself (e.g., authentication, access control) is **out of scope** for this example.
- The OAuth2 server included here is for **development purposes only**. In production, use a real identity provider (Keycloak, Auth0, Okta, etc.).

---

## Variations to try

- **Custom spans:** add manual instrumentation to trace specific operations
- **Metrics:** configure OpenTelemetry metrics alongside traces
- **Logs:** add log export to the OTel collector
- **Sampling:** configure trace sampling to reduce overhead in production
- **Different exporters:** replace Jaeger with Grafana Tempo, Datadog, or other backends
- **Production setup:** replace dev OAuth2 server with real identity provider
- **Multi-collector:** set up multiple collectors for different environments
- **Trace context propagation:** observe how context flows across service boundaries

---

## Next steps

- Explore `security/gated` and `security/advanced` for production-ready security patterns
- Add custom OpenTelemetry attributes to enrich traces with business context
- Implement metrics collection alongside tracing
- Set up alerting based on trace data
- Configure distributed tracing across multiple biomes or peer topologies
- Integrate with production APM platforms (Datadog, New Relic, Dynatrace)

---

This example demonstrates **production-grade observability** for Naylence applications, showing how distributed traces flow through the fabric with proper authentication and security — essential for monitoring complex multi-agent systems in production.
